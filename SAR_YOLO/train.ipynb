{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-09T09:21:46.553748800Z",
     "start_time": "2024-05-09T09:21:35.906356200Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# FILE = Path(__file__).resolve()\n",
    "ROOT = os.getcwd()  # YOLOv5 root directory\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
    "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
    "\n",
    "from utils.general import (\n",
    "    LOGGER,\n",
    "    TQDM_BAR_FORMAT,\n",
    "    check_amp,\n",
    "    check_dataset,\n",
    "    check_file,\n",
    "    check_git_info,\n",
    "    check_git_status,\n",
    "    check_img_size,\n",
    "    check_requirements,\n",
    "    check_suffix,\n",
    "    check_yaml,\n",
    "    colorstr,\n",
    "    get_latest_run,\n",
    "    increment_path,\n",
    "    init_seeds,\n",
    "    intersect_dicts,\n",
    "    labels_to_class_weights,\n",
    "    labels_to_image_weights,\n",
    "    methods,\n",
    "    one_cycle,\n",
    "    print_args,\n",
    "    print_mutation,\n",
    "    strip_optimizer,\n",
    "    yaml_save,\n",
    ")\n",
    "\n",
    "LOCAL_RANK = int(os.getenv(\"LOCAL_RANK\", -1))  # https://pytorch.org/docs/stable/elastic/run.html\n",
    "RANK = int(os.getenv(\"RANK\", -1))\n",
    "WORLD_SIZE = int(os.getenv(\"WORLD_SIZE\", 1))\n",
    "GIT_INFO = check_git_info()\n",
    "\n",
    "def parse_opt(known=False):\n",
    "    \"\"\"Parses command-line arguments for YOLOv5 training, validation, and testing.\"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--weights\", type=str, default=ROOT / \"yolov5s.pt\", help=\"initial weights path\")\n",
    "    parser.add_argument(\"--cfg\", type=str, default=\"\", help=\"model.yaml path\")\n",
    "    parser.add_argument(\"--data\", type=str, default=ROOT / \"data/SRSDD.yaml\", help=\"dataset.yaml path\")\n",
    "    parser.add_argument(\"--hyp\", type=str, default=ROOT / \"data/hyps/hyp.scratch-low.yaml\", help=\"hyperparameters path\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=30, help=\"total training epochs\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=16, help=\"total batch size for all GPUs, -1 for autobatch\")\n",
    "    parser.add_argument(\"--imgsz\", \"--img\", \"--img-size\", type=int, default=640, help=\"train, val image size (pixels)\")\n",
    "    parser.add_argument(\"--rect\", action=\"store_true\", help=\"rectangular training\")\n",
    "    parser.add_argument(\"--resume\", nargs=\"?\", const=True, default=False, help=\"resume most recent training\")\n",
    "    parser.add_argument(\"--nosave\", action=\"store_true\", help=\"only save final checkpoint\")\n",
    "    parser.add_argument(\"--noval\", action=\"store_true\", help=\"only validate final epoch\")\n",
    "    parser.add_argument(\"--noautoanchor\", action=\"store_true\", help=\"disable AutoAnchor\")\n",
    "    parser.add_argument(\"--noplots\", action=\"store_true\", help=\"save no plot files\")\n",
    "    parser.add_argument(\"--evolve\", type=int, nargs=\"?\", const=300, help=\"evolve hyperparameters for x generations\")\n",
    "    parser.add_argument(\n",
    "        \"--evolve_population\", type=str, default=ROOT / \"data/hyps\", help=\"location for loading population\"\n",
    "    )\n",
    "    parser.add_argument(\"--resume_evolve\", type=str, default=None, help=\"resume evolve from last generation\")\n",
    "    parser.add_argument(\"--bucket\", type=str, default=\"\", help=\"gsutil bucket\")\n",
    "    parser.add_argument(\"--cache\", type=str, nargs=\"?\", const=\"ram\", help=\"image --cache ram/disk\")\n",
    "    parser.add_argument(\"--image-weights\", action=\"store_true\", help=\"use weighted image selection for training\")\n",
    "    parser.add_argument(\"--device\", default=\"\", help=\"cuda device, i.e. 0 or 0,1,2,3 or cpu\")\n",
    "    parser.add_argument(\"--multi-scale\", action=\"store_true\", help=\"vary img-size +/- 50%%\")\n",
    "    parser.add_argument(\"--single-cls\", action=\"store_true\", help=\"train multi-class data as single-class\")\n",
    "    parser.add_argument(\"--optimizer\", type=str, choices=[\"SGD\", \"Adam\", \"AdamW\"], default=\"SGD\", help=\"optimizer\")\n",
    "    parser.add_argument(\"--sync-bn\", action=\"store_true\", help=\"use SyncBatchNorm, only available in DDP mode\")\n",
    "    parser.add_argument(\"--workers\", type=int, default=0, help=\"max dataloader workers (per RANK in DDP mode)\")\n",
    "    parser.add_argument(\"--project\", default=ROOT / \"runs/train\", help=\"save to project/name\")\n",
    "    parser.add_argument(\"--name\", default=\"exp\", help=\"save to project/name\")\n",
    "    parser.add_argument(\"--exist-ok\", action=\"store_true\", help=\"existing project/name ok, do not increment\")\n",
    "    parser.add_argument(\"--quad\", action=\"store_true\", help=\"quad dataloader\")\n",
    "    parser.add_argument(\"--cos-lr\", action=\"store_true\", help=\"cosine LR scheduler\")\n",
    "    parser.add_argument(\"--label-smoothing\", type=float, default=0.0, help=\"Label smoothing epsilon\")\n",
    "    parser.add_argument(\"--patience\", type=int, default=100, help=\"EarlyStopping patience (epochs without improvement)\")\n",
    "    parser.add_argument(\"--freeze\", nargs=\"+\", type=int, default=[0], help=\"Freeze layers: backbone=10, first3=0 1 2\")\n",
    "    parser.add_argument(\"--save-period\", type=int, default=-1, help=\"Save checkpoint every x epochs (disabled if < 1)\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=0, help=\"Global training seed\")\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"Automatic DDP Multi-GPU argument, do not modify\")\n",
    "\n",
    "    # Logger arguments\n",
    "    parser.add_argument(\"--entity\", default=None, help=\"Entity\")\n",
    "    parser.add_argument(\"--upload_dataset\", nargs=\"?\", const=True, default=False, help='Upload data, \"val\" option')\n",
    "    parser.add_argument(\"--bbox_interval\", type=int, default=-1, help=\"Set bounding-box image logging interval\")\n",
    "    parser.add_argument(\"--artifact_alias\", type=str, default=\"latest\", help=\"Version of dataset artifact to use\")\n",
    "\n",
    "    # NDJSON logging\n",
    "    parser.add_argument(\"--ndjson-console\", action=\"store_true\", help=\"Log ndjson to console\")\n",
    "    parser.add_argument(\"--ndjson-file\", action=\"store_true\", help=\"Log ndjson to file\")\n",
    "\n",
    "    return parser.parse_known_args()[0] if known else parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1m4255116966: \u001B[0mweights=yolov5s.pt, cfg=, data=data\\SRSDD.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=30, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data\\hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=0, project=runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mgithub: \u001B[0mskipping check (offline), for updates see https://github.com/ultralytics/yolov5\n"
     ]
    }
   ],
   "source": [
    "# 获取参数\n",
    "opt= parse_opt(True)\n",
    "\n",
    "if RANK in {-1, 0}:\n",
    "    print_args(vars(opt))       # 输出参数\n",
    "    check_git_status()          # 检查git状态"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T09:21:56.602877600Z",
     "start_time": "2024-05-09T09:21:46.554744600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\SRSDD.yaml\n",
      "\n",
      "data\\hyps\\hyp.scratch-low.yaml\n",
      "yolov5s.pt\n",
      "runs\\train\n"
     ]
    }
   ],
   "source": [
    "opt.data, opt.cfg, opt.hyp, opt.weights, opt.project = (\n",
    "        check_file(opt.data),\n",
    "        check_yaml(opt.cfg),\n",
    "        check_yaml(opt.hyp),\n",
    "        str(opt.weights),\n",
    "        str(opt.project),\n",
    "    )\n",
    "print(opt.data)\n",
    "print(opt.cfg)\n",
    "print(opt.hyp)\n",
    "print(opt.weights)\n",
    "print(opt.project)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T09:21:56.647934300Z",
     "start_time": "2024-05-09T09:21:56.603851100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from utils.loggers.comet.comet_utils import check_comet_resume\n",
    "from utils.downloads import is_url\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "# Resume judge\n",
    "if opt.resume and not check_comet_resume(opt) and not opt.evolve:\n",
    "    last = Path(check_file(opt.resume) if isinstance(opt.resume, str) else get_latest_run())\n",
    "    opt_yaml = last.parent.parent / \"opt.yaml\"  # train options yaml\n",
    "    opt_data = opt.data  # original dataset\n",
    "    if opt_yaml.is_file():\n",
    "        with open(opt_yaml, errors=\"ignore\") as f:\n",
    "            d = yaml.safe_load(f)\n",
    "    else:\n",
    "        d = torch.load(last, map_location=\"cpu\")[\"opt\"]\n",
    "    opt = argparse.Namespace(**d)  # replace\n",
    "    opt.cfg, opt.weights, opt.resume = \"\", str(last), True  # reinstate\n",
    "    if is_url(opt_data):\n",
    "        opt.data = check_file(opt_data)  # avoid HUB resume auth timeout\n",
    "else:   # 新建训练\n",
    "    opt.data, opt.cfg, opt.hyp, opt.weights, opt.project = (\n",
    "        check_file(opt.data),# 检查文件是否存在，不存在则报错或者下载\n",
    "        check_yaml(opt.cfg),\n",
    "        check_yaml(opt.hyp),\n",
    "        str(opt.weights),\n",
    "        str(opt.project),\n",
    "    )  # checks\n",
    "    assert len(opt.cfg) or len(opt.weights), \"either --cfg or --weights must be specified\"\n",
    "    if opt.evolve:\n",
    "        if opt.project == str(ROOT / \"runs/train\"):  # if default project name, rename to runs/evolve\n",
    "            opt.project = str(ROOT / \"runs/evolve\")\n",
    "        opt.exist_ok, opt.resume = opt.resume, False  # pass resume to exist_ok and disable resume\n",
    "    if opt.name == \"cfg\":\n",
    "        opt.name = Path(opt.cfg).stem  # use model.yaml as name\n",
    "    opt.save_dir = str(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))     # 如果path存在会自动增加后缀\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T09:21:58.869172500Z",
     "start_time": "2024-05-09T09:21:56.624549800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  299d3d7 Python-3.8.18 torch-1.13.1+cu116 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DDP mode（Distributed Data Parallel）分布式训练（直接无视）\n",
    "from utils.torch_utils import select_device\n",
    "import torch.distributed as dist\n",
    "device = select_device(opt.device, batch_size=opt.batch_size)\n",
    "if LOCAL_RANK != -1:        # 如果 LOCAL_RANK 不等于 -1，那么代码就会进入分布式训练模式\n",
    "    msg = \"is not compatible with YOLOv5 Multi-GPU DDP training\"\n",
    "    assert not opt.image_weights, f\"--image-weights {msg}\"\n",
    "    assert not opt.evolve, f\"--evolve {msg}\"\n",
    "    assert opt.batch_size != -1, f\"AutoBatch with --batch-size -1 {msg}, please pass a valid --batch-size\"\n",
    "    assert opt.batch_size % WORLD_SIZE == 0, f\"--batch-size {opt.batch_size} must be multiple of WORLD_SIZE\"\n",
    "    assert torch.cuda.device_count() > LOCAL_RANK, \"insufficient CUDA devices for DDP command\"\n",
    "    torch.cuda.set_device(LOCAL_RANK)\n",
    "    device = torch.device(\"cuda\", LOCAL_RANK)\n",
    "    dist.init_process_group(\n",
    "        backend=\"nccl\" if dist.is_nccl_available() else \"gloo\", timeout=timedelta(seconds=10800)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T09:21:58.979210600Z",
     "start_time": "2024-05-09T09:21:58.874155700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_dir: runs\\train\\exp4\n",
      "epochs: 30\n",
      "batch_size: 16\n",
      "weights: yolov5s.pt\n",
      "single_cls: False\n",
      "evolve: None\n",
      "data: data\\SRSDD.yaml\n",
      "cfg: \n",
      "resume: False\n",
      "noval: False\n",
      "nosave: False\n",
      "workers: 0\n",
      "freeze: [0]\n"
     ]
    }
   ],
   "source": [
    "save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze = (\n",
    "        Path(opt.save_dir),\n",
    "        opt.epochs,\n",
    "        opt.batch_size,\n",
    "        opt.weights,\n",
    "        opt.single_cls,\n",
    "        opt.evolve,\n",
    "        opt.data,\n",
    "        opt.cfg,\n",
    "        opt.resume,\n",
    "        opt.noval,\n",
    "        opt.nosave,\n",
    "        opt.workers,\n",
    "        opt.freeze,\n",
    "    )\n",
    "print('save_dir:',save_dir)\n",
    "print('epochs:',epochs)\n",
    "print('batch_size:',batch_size)\n",
    "print('weights:',weights)\n",
    "print('single_cls:',single_cls)\n",
    "print('evolve:',evolve)\n",
    "print('data:',data)\n",
    "print('cfg:',cfg)\n",
    "print('resume:',resume)\n",
    "print('noval:',noval)\n",
    "print('nosave:',nosave)\n",
    "print('workers:',workers)\n",
    "print('freeze:',freeze)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T09:21:59.023608800Z",
     "start_time": "2024-05-09T09:21:58.984150300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights_dir: runs\\train\\exp4\\weights\n",
      "last pt: runs\\train\\exp4\\weights\\last.pt\n",
      "best pt: runs\\train\\exp4\\weights\\best.pt\n"
     ]
    }
   ],
   "source": [
    "w = save_dir / \"weights\"  # weights dir\n",
    "(w.parent if evolve else w).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "last, best = w / \"last.pt\", w / \"best.pt\"\n",
    "\n",
    "print('weights_dir:',w)\n",
    "print('last pt:',last)\n",
    "print('best pt:',best)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T09:21:59.024616Z",
     "start_time": "2024-05-09T09:21:58.996837100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mhyperparameters: \u001B[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n"
     ]
    }
   ],
   "source": [
    "hyp = opt.hyp   # hyperparameters\n",
    "if isinstance(hyp, str):\n",
    "    with open(hyp, errors=\"ignore\") as f:\n",
    "        hyp = yaml.safe_load(f)  # load hyps dict\n",
    "LOGGER.info(colorstr(\"hyperparameters: \") + \", \".join(f\"{k}={v}\" for k, v in hyp.items()))\n",
    "opt.hyp = hyp.copy()  # for saving hyps to checkpoints"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T09:21:59.038900200Z",
     "start_time": "2024-05-09T09:21:59.011628900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "if not evolve:\n",
    "    yaml_save(save_dir / \"hyp.yaml\", hyp)\n",
    "    yaml_save(save_dir / \"opt.yaml\", vars(opt))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T09:21:59.041893400Z",
     "start_time": "2024-05-09T09:21:59.026604Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mComet: \u001B[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5  runs in Comet\n",
      "\u001B[34m\u001B[1mTensorBoard: \u001B[0mStart with 'tensorboard --logdir runs\\train', view at http://localhost:6006/\n"
     ]
    }
   ],
   "source": [
    "from utils.loggers import LOGGERS, Loggers\n",
    "from utils.callbacks import Callbacks\n",
    "callbacks = Callbacks()\n",
    "\n",
    "callbacks.run(\"on_pretrain_routine_start\")\n",
    "# Loggers\n",
    "data_dict = None\n",
    "if RANK in {-1, 0}:\n",
    "    include_loggers = list(LOGGERS)\n",
    "    if getattr(opt, \"ndjson_console\", False):\n",
    "        include_loggers.append(\"ndjson_console\")\n",
    "    if getattr(opt, \"ndjson_file\", False):\n",
    "        include_loggers.append(\"ndjson_file\")\n",
    "\n",
    "    loggers = Loggers(\n",
    "        save_dir=save_dir,\n",
    "        weights=weights,\n",
    "        opt=opt,\n",
    "        hyp=hyp,\n",
    "        logger=LOGGER,\n",
    "        include=tuple(include_loggers),\n",
    "    )\n",
    "\n",
    "    # Register actions\n",
    "    for k in methods(loggers):\n",
    "        callbacks.register_action(k, callback=getattr(loggers, k))\n",
    "\n",
    "    # Process custom dataset artifact link\n",
    "    data_dict = loggers.remote_dataset\n",
    "    if resume:  # If resuming runs from remote artifact\n",
    "        weights, epochs, hyp, batch_size = opt.weights, opt.epochs, opt.hyp, opt.batch_size\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T09:21:59.101183400Z",
     "start_time": "2024-05-09T09:21:59.044880800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class names: {0: 'Container', 1: 'ore-oil', 2: 'Dredger', 3: 'LawEnforc', 4: 'Cell-Container', 5: 'Fishing'}\n"
     ]
    }
   ],
   "source": [
    "from utils.torch_utils import torch_distributed_zero_first\n",
    " # Config\n",
    "plots = not evolve and not opt.noplots  # create plots\n",
    "cuda = device.type != \"cpu\"\n",
    "init_seeds(opt.seed + 1 + RANK, deterministic=True)\n",
    "with torch_distributed_zero_first(LOCAL_RANK):\n",
    "    data_dict = data_dict or check_dataset(data)  # check if None\n",
    "train_path, val_path = data_dict[\"train\"], data_dict[\"val\"]\n",
    "nc = 1 if single_cls else int(data_dict[\"nc\"])  # number of classes\n",
    "names = {0: \"item\"} if single_cls and len(data_dict[\"names\"]) != 1 else data_dict[\"names\"]  # class names\n",
    "is_coco = isinstance(val_path, str) and val_path.endswith(\"coco/val2017.txt\")  # COCO dataset\n",
    "\n",
    "print('class names:', names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T09:21:59.112146700Z",
     "start_time": "2024-05-09T09:21:59.059327400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=6\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     29667  models.yolo.Detect                      [6, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7035811 parameters, 7035811 gradients, 16.0 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "\u001B[34m\u001B[1mAMP: \u001B[0mchecks passed \n"
     ]
    }
   ],
   "source": [
    "from utils.downloads import attempt_download\n",
    "from models.yolo import Model\n",
    "\n",
    "# Model\n",
    "check_suffix(weights, \".pt\")  # check weights\n",
    "pretrained = weights.endswith(\".pt\")\n",
    "if pretrained:              # 如果是预训练模型\n",
    "    with torch_distributed_zero_first(LOCAL_RANK):          # 仅在LOCAL_RANK为0时执行（分布式训练可能会被执行多次如果不这样的话）\n",
    "        weights = attempt_download(weights)  # download if not found locally\n",
    "    ckpt = torch.load(weights, map_location=\"cpu\")  # load checkpoint to CPU to avoid CUDA memory leak\n",
    "    model = Model(cfg or ckpt[\"model\"].yaml, ch=3, nc=nc, anchors=hyp.get(\"anchors\")).to(device)  # create\n",
    "    exclude = [\"anchor\"] if (cfg or hyp.get(\"anchors\")) and not resume else []  # exclude keys\n",
    "    csd = ckpt[\"model\"].float().state_dict()  # checkpoint state_dict as FP32\n",
    "    csd = intersect_dicts(csd, model.state_dict(), exclude=exclude)  # intersect\n",
    "    model.load_state_dict(csd, strict=False)  # load\n",
    "    LOGGER.info(f\"Transferred {len(csd)}/{len(model.state_dict())} items from {weights}\")  # report\n",
    "else:\n",
    "    model = Model(cfg, ch=3, nc=nc, anchors=hyp.get(\"anchors\")).to(device)  # create\n",
    "amp = check_amp(model)  # check AMP"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T09:22:05.137540Z",
     "start_time": "2024-05-09T09:21:59.072278800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
